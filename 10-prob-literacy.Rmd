# What is Probability? {#prob-literacy}



This chapter provides a non-technical introduction to randomness and probability.  Many of the topics introduced in this chapter will be covered in more detail in later chapters.



NO MATH BEYOND TWO WAY TABLES AND BASIC EXPECTED VALUES

Expand chapter 1 with more example of probability/simulation computations with as little math and terminology as possible.  (e.g. Don't define RV or event or sample space.)

Add a few motivating examples with links - 538 election, hurricane, etc




## Instances of randomness {#randomness}

instances of randomness - examples; include in other sections? (random sampling, random assignment, future/past, etc, physical (coin, die), quantum)



```{exercise randomness}

Each of the following situations involves a probability. How are the various situations similar, and how are they different?  What is one feature that all of the situations have in common? If you were to estimate the probability in question, how might you do it?  What are some things to consider?  The goal here is not to do any calculations but rather to think about, via these examples, similarities and differences of situations in which probabilities are of interest.

```
	
1. The probability that a single flip of a fair coin lands on heads.
1. The probability that in two flips of a fair coin exactly one flip lands on heads.
1. The probability that in 10000 flips of a fair coin exactly 5000 flips land on heads.
1. The probability that in 10000 flips of a fair coin “around” 5000 flips land on heads.
1.  The probability you win the next [Powerball lottery](https://www.powerball.com/) if you purchase a single ticket, 6-7-16-23-26, plus the Powerball number, 4. (FYI: There are roughly^[The exact count is 292,201,338.  We will see how to compute this number later.] 300 million possible winning number combinations.)
1. The probability you win the next Powerball lottery if you purchase a single ticket, 1-2-3-4-5, plus the Powerball number, 6.
1. The probability that someone wins the next Powerball lottery.  (FYI: especially when the jackpot is large, there are hundreds of millions of tickets sold.)
1. The probability that a “randomly selected” Cal Poly student is from CA.  
1. The probability that Hurricane Humberto makes landfall in the U.S.
1. The probability that the Los Angeles Chargers win the next Superbowl.
1. The probability that Donald Trump wins the 2020 U.S. Presidential Election.
1. The probability that extraterrestrial life currently exists somewhere in the universe.
1. The probability that you ate an apple on April 17, 2009.



- The subject of probability concerns *random* phenomena.
- A phenomenon is **random** if there are multiple potential outcomes, and there is **uncertainty** about which outcome will occur.  *Uncertainty* is the feature that all the scenarios have in common.
- Uncertainty does not necessarily mean uncertainty about an occurrence in the future. For example, you either ate or apple or not on April 17, 2009, but you're not certain about it.  But if you tended to eat a lot of apples ten years ago, then you might give a high probability to the event that you ate one on April 17, 2009.
- Many phenomena involve physical randomness^[We will refer to as "random" any scenario that involves a reasonable degree of uncertainty.  We're avoiding philosophical questions about what is "true" randomness, like the following.  Is a coin flip really random? If all factors that affect the trajectory of the coin were known precisely, then wouldn't the outcome be determined?  Does true randomness only exist in quantum mechanics?], like flipping a coin or drawing powerballs at random from a bin.
- Statistical applications often involve the planned use of physical randomness
  - **Random selection** involves selecting a *sample* of individuals at random from a *population* (e.g. via random digit dialing).
  - **Random assignment** involves assigning individuals at random to groups (e.g. in a randomized experiment).
- However, in many other situations randomness just vaguely reflects uncertainty.
- In any case, random does *not* mean haphazard. In a random phenomenon, while
individual outcomes are uncertain, there is a *regular distribution of
outcomes over a large number of (hypothetical) repetitions*.
  - In two flips of a fair coin we wouldn't necessarily see one head and one tail. But in 10000 flips of a fair coin, we would expect to see close to 5000 heads and 5000 tails. 
  - We don't know who will win the next Superbowl, but we can and should certainly consider some teams as more likely to win than others.  We could imagine a large number of hypothetical 2019 seasons; how often would we expect the Eagles to win? The Raiders? (Hopefully a lot for the Eagles; probably not much for the Raiders).
- Also, random does *not* necessarily mean equally likely. In a random
phenomenon, certain outcomes or events might be more or less likely than
others.
  - It's much more likely that a randomly selected Cal Poly student is from CA than not.
  - Not all NFL teams are equally likely to win the next Superbowl.


## Interpretations of probability {#interpretations}


In the previous section we encountered a variety of scenarios which involved uncertainty, a.k.a. randomness.  Just as there are a few "types" of randomness, there are a few ways of interpreting probability, namely, *long run relative frequency* and *subjective probability*.

```{exercise probability-intepret}

Revisit the scenarios in Exercise \@ref(exr:randomness).  Now consider how "probability" is interpreted in the different scenarios.  In each scenario, what does "probability" mean?  How might you estimate the probability?  Start to make guesses for the probabilities; are they "high" or "low"?  How high or low? Again, the goal is not to do any calculations but rather to think about, via these examples, similarities and differences of situations in which probabilities are of interest.

```

In particular, compare

a. Scenarios 3 and 4
a. Scenarios 5 and 6
a. Scenarios 6 and 7
a. How are scenarios 1 through 8 (collectively) different from scenarios 9 through 13 (collectively)?



- The **probability** of an event is a number in the interval $[0, 1]$ measuring the event's likelihood or degree of uncertainty.
- A probability can take any values in the continuous scale from 0% to 100%^[Probabilities are usually defined as decimals, but are often colloquially referred to as percentages.  We're not sticklers; we'll refer to probabilities as decimals and as percentages.]. In particular, a probability requires much more interpretation than "is the probability greater than, less than, or equal to 50%?"
- When interpreting probabilities, be careful not to confuse "the particular" with "the general".
  - ("The particular.") A very specific event, surprising or not, often has low probability.
    - Even though in 10000 flips of a fair coin we would expect to see about 5000 heads, the probability that *exactly* 5000 out of 10000 flips are heads is fairly small (about^[We will see how to compute probabilities like this one in upcoming chapters.] 0.008).
    - The probability that the winning powerball number is 6-7-16-23-26-(4) is exactly the same as the probability that the winning powerball number is 1-2-3-4-5-(6).  Each of these sequences is just one of the roughly 300 million possible sequences, and each sequences has about a 1 in 300 million chance of being the winning number.  However, many people think 6-7-16-23-26-(4) is more likely because 1-2-3-4-5-(6) "doesn't look random".
    - The probability that you get a text from your best friend at 7:43pm on Oct 12, 2019 inviting you to dinner after you've just ordered pizza from your favorite pizza place is probably pretty small.  None of these items --- getting a text, having a friend invite you to dinner, ordering pizza from your favorite pizza place --- is unusual, but the chances of them all combining in this way at this particular time are fairly small. 
  - ("The general.") However, if there are many like events, their combined probability can be high.
    - The probability that *around* 5000 out of 10000 coin flips land on heads is fairly large.  For example, if "around" is interpreted as between 4900 and 5100 (for a proportion of heads between 0.49 to 0.51) the probability is about 0.956.
    - The probability that the winning powerball number is an ordered sequence, like 1-2-3-4-5-(6), is extremely small^[If a "sequence" is defined with the powerball as the last number in the sequence, then the probability is about 21 out of 300 million.  Since the powerball must be a number from 1 to 26, there are only 21 tickets out of 300 million possibilities for which the numbers are in an ordered sequence: 1-2-3-4-5-(6), 2-3-4-5-6-(7), ... 21-22-23-24-25-(26).].  However, the probability that the winning number is not an ordered sequence, like 6-7-16-23-26-(4), is exremely high.  When interpreting probabilities, be careful not to confuse an event like "the winning number is 6-7-16-23-26-(4)" (the particular, low probability) with an event like "the winning number is not an ordered sequence" (the general, high probability).  
    - The probability that some time in the next month or so a friend invites you for dinner after you've already had dinner on your own is probably fairly high.
- Even if an event has extremely small probability, given enough
repetitions of the random phenomenon, the probability that the event occurs
on *at least one* of the repetitions is high.
  - The probability that a specific powerball ticket is the winning number is about 1 in 300 million.  So if you buy a single ticket, it is [extremely unlikely](https://www.huffpost.com/entry/chances-of-winning-powerball-lottery_b_3288129) that *you* will win.
  - However, if hundreds of millions of powerball tickets are sold, the probability that *someone somewhere* wins is pretty high.  For example, if 500 million tickets are sold then there is a roughly 80% chance that at least one ticket has the winning number (under [certain assumptions](https://fivethirtyeight.com/features/new-powerball-odds-could-give-america-its-first-billion-dollar-jackpot/)). 


### Relative frequency {#rel-freq}

The probability that a single flip of a fair coin lands on heads is 0.5.  How do we interpret this 0.5?  The notation of "fairness" implies that the two outcomes, heads and tails, should be equally likely, so we have a "50/50 chance".  But how else can we interpret this 50%?  One way is by considering *what would happen if we flipped the coin main times*.  Now, if we would flipped the coin twice, we wouldn't expect to necessarily see one head and one tail.  And we already mentioned that if we flipped the coin 10000 times, the chances of seeing *exactly* 5000 heads is small.  But in many flips, we might expect to see heads on something close to 50\% of flips.

Consider Figure \@ref(fig:coin-sim1) below.  Each dot represents a set of 10,000 fair coin flips.  There are 100 dots displayed, representing 100 different sets of 10,000 coin flips each.  For each set of flips, the proportion of the 10,000 flips which landed on head is recorded.  For example, if in one set 4973 out of 10,000 flips landed on heads, the proportion of heads is 0.4973.  The plot displays 100 such proportions.  We see that only 5 of these 100 proportions are less than 0.49 or greater than 0.51.  So if between 0.49 and 0.51 is considered "close to 0.5", then yes, in 10000 coin flips we would expect the proportion of heads to be close to 0.5.  (In 10000 flips, the probability of heads on between 49\% and 51\% of flips is 0.956, so 95 out of 100 provides a rough estimate of this probability.)    

(ref:cap-coin-sim1) Proportion of flips which are heads in 100 sets of **10,000** fair coin flips.  Each dot represents a set of **10,000** fair coin flips.


```{r coin-sim1, echo=FALSE, fig.cap="(ref:cap-coin-sim1)"}

knitr::include_graphics("_graphics/coin-sim1.png")

```

But what if we want to be stricter about what qualifies as "close to 0.5"?  You might suspect that with even more flips we would expect to observe heads on even closer to 50\% of flips.  Indeed, this is the case.  Figure \@ref(fig:coin-sim2) displays the results of 100 sets of 1,000,000 fair coin flips.  The pattern seems similar to Figure \@ref(fig:coin-sim1) but pay close attention to the horizontal axis which covers a much shorter range of values than in the previous figure.  Now 96 of the 100 proportions are between 0.499 and 0.501. So in 1,000,000 flips we would expect the proportion of heads to be between 0.499 and 0.501, pretty close to 0.5. (In 1,000,000 flips, the probability of heads on between 49.9\% and 50.1\% of flips is 0.955, and 96 out of 100 sets provides a rough estimate of this probability.) 


(ref:cap-coin-sim2) Proportion of flips which are heads in 100 sets of **1,000,000** fair coin flips. Each dot represents a set of **1,000,000** fair coin flips.



```{r coin-sim2, echo=FALSE, fig.cap="(ref:cap-coin-sim2)"}

knitr::include_graphics("_graphics/coin-sim2.png")

```


In Figure \@ref(fig:coin-sim3) each dot represents a set of  100 millions flips.  The pattern seems similar to the previous figures, but again pay close attention the horizontal access which covers a smaller range of values. Now 96 of the 100 proportions are between 0.4999 and 0.5001. (In 100 million flips, The probability of heads on between 49.99\% and 50.01\% of flips is 0.977, so 96 out of 100 sets provides a rough estimate of this probability.)


(ref:cap-coin-sim3) Proportion of flips which are heads in 100 sets of **100,000,000** fair coin flips. Each dot represents a set of **100,000,000** fair coin flips.


```{r coin-sim3, echo=FALSE, fig.cap="(ref:cap-coin-sim3)"}

knitr::include_graphics("_graphics/coin-sim3.png")

```

The previous figures illustrate that the more flips there are, the more likely it is that we observe a proportion of flips landing on heads close to 0.5.  We also see that with more flips we can refine our definition of "close to 0.5": increasing the number of flips by a factor of 100 (10,000 to 1,000,000 to 100,000,000) seems to give us an additional decimal place of precision ($0.5\pm0.01$ to $0.5\pm 0.001$ to $0.5\pm 0.0001$.)  These observations illustrate the relative frequency interpretation of probability.  




- The probability of an event corresponding to the result of a random phenomenon can be interpreted as the proportion of times that the event would occur in a very large number of hypothetical repetitions of the random phenomenon.
- That is, a probability can be interpreted as a **long run proportion** or **long run relative frequency**.
- This means that the probability of an event can be approximated by **simulating** the random phenomenon a large number of times and determining the proportion of simulated repetitions on which the event occurred out of the total number of repetitions of the simulation (this proportion is also called the relative frequency of the event.)
  - A **simulation** involves an artificial recreation of the random phenomenon, usually using a computer.
  - For example, if a basketball player is successful on 90\% of her free throw attempts, we can simulate the player shooting a single free throw attempt by taking 10 cards and labeling 9 as "success" and 1 as "miss" then shuffling well  and dealing one card.
- The **long run relative frequency** interpretation of probability can be applied when a situation can be repeated numerous times, at least conceptually, and the outcome can be observed each time.
- The relative frequency of a particular event will settle down to a single constant value after many repetitions, and that long run value is the probability of that event.
  - However, what constitutes the random phenomenon or how the simulation is conducted depends on certain assumptions.  Changing those assumptions can affect probabilities of interest.
    - For example, if you're interested in the probability that a die lands on 1, you need to know if it's a four-sided die or a six-sided die, and if the die is consider "fair".
    - As a more complicated example, simulating the outcome of the next Superbowl involves many assumptions

### Subjective probability

The relative frequency interpretation is natural in scenarios 1 through 8 of Exercise \@ref(exr:randomness).  We can consider as repeateable situations like flipping a coin, drawing powerballs from a bin, or selecting a student at random.

On the other hand, it is difficult to conceptualize scenarios 8 through 13 of Exercise \@ref(exr:randomness) as relative frequencies.  Superbowl 2020 will only be played once, the 2020 U.S. Presidential Election will only be conducted once (we hope), and there was only one April 17, 2009 on which you either did or did not eat an apple.  But while these situations are not naturally repeatable they still involve randomness (uncertainty) and it is still reasonable to assign probabilities.  At this point in time, the Chargers are less likely than the Patriots (ugh) to win Superbowl 2020, Donald Trump is more likely than Dwayne Johnson to win the U.S. 2020 Presidential Election, and if you've always been an "apple-a-day" person, there's a good chance you ate one on April 17, 2009.  So it still makes sense to talk about probability in uncertain, but not necessarily repeated situations.

However, the *meaning* of probability does seem different in scenarios 9 through 13 compared to 1 through 8.  Consider Superbowl 2020.  As of Sept 16,

- According to [fivethirtyeight.com](https://projects.fivethirtyeight.com/2019-nfl-predictions/), the Patriots have a 19% chance of winning the Superbowl, the highest of any team, while the Chargers have a 4% chance.
- According to [footballoutsider.com](https://www.footballoutsiders.com/stats/playoffodds), the Patriots have a 26% chance of winning the Superbowl, the highest of any team, while the Chargers have a 6% chance.
- According to [playoffstatus.com](http://www.playoffstatus.com/nfl/nflpostseasonprob.html) the Patriots have a 7% chance of winning the Superbowl, behind the Chiefs and Packers at 9% each, while the Chargers have a 3% chance.

All three websites, as well as many others, ascribe different probabilities to the Patriots or Chargers winning. Which website, if any, is correct?  In the coin flipping example, we could perform a simulation to see that the long run relative frequency is 0.5.  However, simulating Superbowl 2020 involves first simulating the 2019 season to determine the playoff matchups, then simulating the playoffs to see which teams make the Superbowl, then simulating the Superbowl matchup itself.  And simulating the 2019 involves simulating all the weekly matchups and potential injuries and their effects.  Even just simulating a single game involves many assumptions; differences in opinions with regards to these assumptions can lead to different probabilities.   For example, according to fivethirtyeight, the Chiefs have a 69% chance of beating the Ravens on Sept 22, but according to pickingpros.com it's only 52%.  Unlike in the coin flipping problem, there is no single set of rules for running the simulation, and there is no single relative frequency that determines the probability.  Therefore, in a situation like forecasting the Superbowl we consider *subjective probability*.

- There are many situations where the outcome is uncertain, but it does mnot make sense to consider the situation as repeatable. In such situations, a **subjective (a.k.a. personal) probability** describes the degree of likelihood a given individual ascribes to a certain event.
- As the name suggests, different individuals might have different subjective (personal) probabilities for the same event.
  - In contrast to the long run relative frequency situation, in which the probability is agreed to be defined as the long run relative frequency.
- The [fivethirtyeight NFL predictions](https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/) are the output of a probabilistic forecast.
  - **Probabilistic forecasts** combine observed data and statistical models to make predictions.
  - Rather than providing a single prediction (such as "the Patriots will win Superbowl 2020"), probabilistic forecasts provide a range of scenarios and their relative likelihoods.
  - Be sure to make a distinction between *assumption* and *observation*.


## Proportional reasoning and tables of counts



- In general, knowing probabilities of individual events alone is not enough to determine probabilities of combinations of them.
- **Two-way tables** (a.k.a. contingency tables) of counts are a useful tool for probability problems dealing with two events.
- For the purposes of constructing the table and computing related probabilities, any value can be used for the hypothetical total count (as long as you don’t round numbers in between).

Suppose we are interested in the relationship between income and college graduation rates at Cal Poly. Consider the sample space $S = \{\text{all current Cal Poly students}\}$.
Suppose that the probability^[These number are estimates based on data from <https://projects.propublica.org/colleges/schools/california-polytechnic-state-university-san-luis-obispo>] that a (randomly selected) Cal Poly student graduates within six years is 0.75, and that the probability that a (randomly selected) Cal Poly student has a Pell grant (for low income students) is 0.2.

1. Do we have enough information to find the probability that a Cal Poly student both has a Pell Grant and graduates within six years?
1. Without knowing any other information, what is the *largest* value the probability in (a) could possibly be?  Under what conditions is this value attained? Set up a two-way table corresponding to this scenario.
1. Without knowing any other information, what is the *smallest* value the probability in (a) could possibly be?  Under what conditions is this value attained? Set up a two-way table corresponding to this scenario.
1. Suppose that the probability that a Cal Poly student both has a Pell Grant and graduates within six years is 0.11. Construct the corresponding two-way table.
1. Find the probability that a Cal Poly student does not have a Pell Grant and graduates within six years.
1. Donny says "75% of CP students graduate within six years while 20% of CP students have a Pell Grant.  So 95% of Cal Poly students graduate within six years or have a Pell Grant." Do you agree?
1. What would need to be true in order for the Donny's statement in the previous part to be true?



## Working with probabilities {#consistency}


In the previous section we saw two different interpretations of probability: relative frequency and subjective.  Fortunately, the mathematics of probability work the same way regardless of the interpretation.  Also, even with subjective probabilities it is helpful to consider what might happen in a simulation.

### Consistency requirements

 With either the relative frequency or personal probability interpretation there are some basic logical consistency requirements^[In Section \@ref(probspace), we will formalize these requirements in the axioms of probability.] which probabilities need to satisfy.



```{example worldseries}

As of Sept 18, the website [fivethirtyeight.com](https://projects.fivethirtyeight.com/2019mlb-predictions/) listed the following probabilities for who will 
win the 2019 World Series.

```

  --------------------- -----
  Houston Astros          25%
  Los Angeles Dodgers     21%
	New York Yankees        20%
	Atlanta Braves           8%
  --------------------- -----

1. Is the relative frequency or subjective probability interpretation more appropriate here?
1. According to the site, what must be the probability that the Houston Astros do *not* win?
1. According to this site, what must be the probability that one of the above four teams is the World Series champion?
1. According to this site, what must be the probability that a team other than the above four teams is the World Series champion?

```{solution worldseries-sol}

to Example \@ref(exm:worldseries)

```


1. It is more appropriate to think of the probabilities themselves in application as subjective.  Different websites or models could reasonably assign other probabilities to the teams.  However, we can still imagine the probabilities as relative frequencies, if it helps our intuition.  If we think of this as a simulation, each repetition results in a World Series champion and in the long run the Astros would be the champion in 25% of repetitions.
1. Either the Astros win or they don't; if there's a 25% chance that the Astros win, there must be a 75% chance that they do not win. If we think of this as a simulation, each repetition results in either the Astros winning or not, so if they win in 25% of repetitions, they must not win in the other 75% to account for 100% of the repetitions.
1. There is only one World Series champion, so if say the Astros win then no other team can win.  Thinking again of a simulation, the repetitions in which the Astros win are distinct from those in which the Dodgers win.  So if the Astros win in 25% of repetitions and the Dodgers wins in 21% repetitions, then on a total of 46% of repetitions either the Astros or Dodgers win.  Adding the four probabilities, we see that the probability that one of the four teams above wins must be 74%.
1. Either one of the four teams above wins, or some other team wins.  If there is a 74% chance that the winner is one of the four teams above, then there must be a 26% chance that the winner is not one of these four teams.





### Odds

The words "probability", "chance", "likelihood", and "odds" are colloquially treated as synonyms.  However, in the mathematical language of probability, *odds* provide a different way of reporting a probability.  Rather than reporting probability on a 0\% to 100\% scale, odds report probabilities in terms of ratios.  


```{example worldseries-odds}

In Example \@ref(exm:worldseries) the odds that the Astros win the World Series are 3 to 1 against.

```


1. What do you think that "3 to 1 against" means?
1. What are the odds of the Astros *not* winning?
1. What are the odds of the Yankees winning?
1. What are the odds of the Braves winning?


```{solution worldseries-odds-sol}

to Example \@ref(exm:worldseries-odds)

```

1. The probability that the Astros win is 0.25, so the probability that they do not win is 0.75.  These numbers are in a 3 to 1 ratio: the probability of not winning (0.75) is 3 times greater than the probability of winning (0.25).  So the odds *against* the Astros winning the World Series are 3 to 1; "against" because the Astros are less likely to win than to not win.
1. The probabilities are still in the 3 to 1 ratio, but we can say that the odds are 3 to 1 *in favor* of the Astros *not* winning.
1. The probability that the Yankees win is 0.2 and that they don't win is 0.8, and $0.8/0.2 = 4$).  So the odds are 4 to 1 against the Yankees winning ("against" because the Yankees are less likely to win than to not win).
1. The probability that the Braves win is 0.08 and that they don't win is 0.92, and $0.92/0.08 = 11.5$).  So the odds are 11.5 to 1, or 23 to 2, against the Braves winning ("against" because the Braves are less likely to win than to not win).


- The **odds** of an event is a ratio involving the probability that the
event occurs and the probability that the event does not occur
$$
\begin{aligned}
\text{odds in favor} & = \frac{\text{probability that the event occurs}}{\text{probability that the event does not occur}} \\
& \\
\text{odds against} & = \frac{\text{probability that the event does not occur}}{\text{probability that the event  occurs}}\end{aligned}
$$
- In many situations (e.g., gambling) odds are implicitly reported as odds against.
- Odds are usually expressed as whole numbers, e.g., 11 to 1, 7 to 2.

Ron and Leslie make the following bet. If Boston wins, Leslie will pay
Ron \$200; if not, Ron will pay Leslie \$100. If both consider this to
be a fair bet, what have they agreed that the probability that Boston
wins is?

The odds of a fair bet on whether or not an event will occur imply a
probability for the event
$$\text{probability that event occurs} = \frac{\text{odds in favor of the event}}{1+\text{odds in favor of the event}}$$





### Dutch book

Dutch book^["Book" in the sense of a bookie taking bets]

Odds give us one way to see why even subjective probabilities most follow basic logical consistency requirements.

```{example dutch}

Donny Dont is pretty sure that the Astros are going to win the World Series.  He thinks their only real competition is the Braves.  The following are Donny's subjective probabilities.

  --------------------- -----
  Houston Astros          60%
  Atlanta Braves          20%
  Other                   10%
  --------------------- -----

```




## Approximating probabilities - a brief introduction to simulation {#sim}

Here's a seemingly simple problem.  Flip a fair coin four times and record the results in order. For the recorded sequence, compute *the proportion of the flips which immediately follow a H that result in H*.  What value do you expect for this proportion? (If there are no flips which immediately follow a H, i.e. the outcome is either TTTT or TTTH, discard the sequence and try again with four more flips.)


For example, the sequence HHTT means the the first and second flips are heads and the third and fourth flips are tails.  For this sequence there are two flips which immediately followed heads, the second and the third, of which one (the second) was heads.  So the proportion in question for this sequence is 1/2.  

So what value do you expect for this proportion? We think it's safe to say that most people would answer 1/2.  Afterall, it shouldn't matter if a flip follows heads or not, right?  We would expect half of the flips to land on heads regardless of whether the flip follows H, right?  We'll see there are some subtleties lurking behind these questions.

To get an idea of what we would expect for this proportion, we could conduct a simulation: flip a coin 4 times and see what happens.  Here are the results of a few repetitions; each repetition consists of an ordered sequence of 4 coin flips for which the proportion in question is measured.  (**Flips which immediately follow H are in bold.**)

Table: (\#tab:mscoin-intro) Simulated outcomes for 6 sets of four flips of a fair coin. 

| Repetition 	|  Outcome 	| Flips that follow H 	| H that follow H 	| Proportion of H following H 	|
|-----------:	|---------:	|---------------------	|-----------------	|----------------------------:	|
|          1 	| H**HT**T 	|                   2 	|               1 	|                         0.5 	|
|          2 	| H**T**TH 	|                   1 	|               0 	|                           0 	|
|          3 	|     TTTH 	|                   0 	|              NA 	|                   try again 	|
|          4 	| TH**HH** 	|                   2 	|               2 	|                           1 	|
|          5 	| H**HT**T 	|                   2 	|               1 	|                         0.5 	|
|          6 	| H**HHT** 	|                   3 	|               2 	|                       0.667 	|

We can keep repeating the above process to investigate what happens in the long run.  Rather than actually flipping coins, we use a computer to run a simulation.  The table and plot below summarize the results of 1,000,000 repetitions of the simulation^[Section XXX covers how to program the simulation and analyze the results.].  While you can't see the individual "dots" in the plot, each dot would represent a sequence of 4 coin flips (with at least one flip following a H) and the value plotted being plotted is the proportion of H following H for that sequence.  The results could be summarized in a table like Table \@ref(tab:mscoin-intro), albeit with 1,000,000 rows (after discarding rows with no flips immediately following H.)


(ref:cap-mscoin-intro) Proportion of flips immediately following Heads that result in Heads for 1,000,000 sets of 4 coin flips.  (Each set has at least one flip immediately following H.) For example, the proportion in 429,123 of sets



```{r ms-coin-intro-plot, echo=FALSE, fig.cap="(ref:cap-mscoin-intro)", out.width='50%', fig.show='hold'}

knitr::include_graphics(c("_graphics/mscoin-intro-table.png", "_graphics/mscoin-intro-plot.png"))

```




We asked the question: *what would you expect* for the proportion of the flips which immediately follow a H that result in H?  That depends on how we define what's "expected".  If we are interested in the value that is most likely to occur when we flip a coin four times, then the answer is 0: we see that in the long run a little over 40\% of the sets resulted in a proportion of 0, while only about 30\% of sets resulted in a value of 1/2.  We see that the plot is not centered at 1/2; a higher percentage of repetitions resulted in a proportion below 1/2 than above 1/2.  We think that most people would find this surprising. 

Another way to interpret "expected" is as "average". Just as probability can be interpreted as long run relative frequency, there is a concept called *expected value* which can be interpreted  as the *long run average value*. After 1,000,000 repetitions, each involving a set of four fair coin flips, we have 1,000,000 simulated values of the proportion of H following H, as recorded in the rightmost column of Table \@ref(tab:mscoin-intro).  We could then average these values: add up the values in the column and divide by 1,000,000.  It turns out that average value is 0.405, which is not 1/2. Again, we think most people find this surprising.

A quick note: the term "expected value" is somewhat of a misnomer.  We are *not* saying that if we flip a coin four times we would expect the proportion of H following H for that set of flips to be 0.405.  In fact, the simulation shows that on any single set of four fair coin flips, the only possible values for the proportion of H following H are 0, 1/2, 2/3, and 1.  So in a set of four coin flips it's not possible to see a proportion of 0.405.  Rather, 0.405 is the *average value of the proportion of H following H that we would expect to see in the long run over many sets of four fair coin flips*.  We will return to this idea later. 

We will return to this example several times throughout the book to investigate these results more closely.   For now, just observe that

- The study of probability can involve some subtleties and our intuition isn't always right.
- Simulation is an effective way of investigating probability problems, and can reveal interesting and suprising patterns.

In MS coin - add statement about probability of H after H versus proportion of H after H



## Sliding scale of probability, or Probability of what?

[Here's a funny series of cartoons](https://mathwithbaddrawings.com/2015/09/23/what-does-probability-mean-in-your-profession/)

What does 1/million mean?

At least one, etc.

xkcd/538 benchmarks, put in benchmarks (one in a million, a billion, etc)

probability of coincidences


## Common misinterpretation and fallacies (e.g. outbreak of Asian disease, Utts book)

Sally Clark?

Allais paradox?

A famous study^[Source: <https://www.ncbi.nlm.nih.gov/pubmed/7455683>] by Kahneman and Tversky presented a group of people
with the following scenario.\
\
"Imagine that the U.S. is preparing for the outbreak of an unusual Asian
disease, which is expected to kill 600 people. Two alternative programs
to combat the disease have been proposed. Assume that the exact
scientific estimate of the consequences of the programs are as follows:

If Program A is adopted, 200 people will be saved.

If Program B is adopted, there is 1/3 probability that 600 people will
be saved, and 2/3 probability that no people will be saved.

Which of the two programs would you favor?"\

Which of these two options, A or B, would you choose? Of the 152
participants, a large majority (72%) favored one of the options; which
one do you think it was?

A second group was presented with the same set up, but the following
options instead of A/B.\

If Program C is adopted 400 people will die.

If Program D is adopted there is 1/3 probability that nobody will die,
and 2/3 probability that 600 people will die.\

Which of these two options, C or D, would you choose? Of the 155
participants, a large majority (78%) favored one of the options; which
one do you think it was?

Compute the expected number of people who die and who are saved with
Program D.

Compute the expected number of people who die and who are saved with
Program B.

Compare options A and C. Are these the same? How about B and D? Do the
risk preferences depend on the wording of how the information is
presented?

Many studies have shown that human intuition does not generally deal
well with issues of uncertainty.

So it's worthwhile to take a careful study



```{example, monty-hall, name='Monty Hall problem'}
MOnty Hall in CHpater 1???
```




## Why study coins, dice, cards, and spinners?


Many probability problems involve “toy” situations like flipping coins, rolling dice, shuffling cards, or spinning spinners.  These situations might seem unexciting, or at least not very practically meaningful.  However, coins and spinners and the like provide familiar, concrete situations which facilitate understanding of probability concepts.  Furthermore, simple situations often provide insight into real and complex problems. The following is just one illustration.

Many basketball players and fans alike believe in the "hot hand"
phenomenon: the idea that making several shots in a row increases a
player's chances of making the next shot. However, the consensus
conclusion of thirty years of studies on the hot hand, beginning with
the seminal study @GVT, had been that there is
no statistical evidence that the hot hand in basketball is real. As a
result, many statisticians regularly caution against the "hot hand
fallacy": the belief that the hot hand exists when, in reality, the
degree of streaky behavior typically observed in sequential data is
consistent with what would be expected simply by chance in independent
trials.

The idea behind studies like @GVT is essentially the
following. Consider a player who attempts 100 shots and makes 50%. If
there is no hot hand, then we might expect the player to make 50% of
shots both on attempts that follow hit streaks --- usually considered
three (or more) made attempts in a row --- and on other attempts.
Therefore, a success rate of 50% on both sets of attempts provides no
evidence of the hot hand.

However, recent research of @MStruth, @MSthree, @MScold concludes that previous studies on the hot hand in basketball,
starting with @GVT, have been subject to a bias. After
correcting for the bias, the authors find strong evidence in favor of
the hot hand effect in basketball shooting, suggesting the hot hand
fallacy is not a fallacy after all. One interesting aspect of these
studies is that Miller and Sanjurjo's methods are simulation-based.

@MStruth introduced the coin flipping problem in Section \@ref(sim) to illustrate the idea behind their research and the
bias in previous studies. Consider again a player who attempts 100 shots
and makes 50%. Even if there is no hot hand, Miller and Sanjurjo show that we would actually expect the player to have a shooting percentage of *strictly less than* 50% on the attempts which followed streaks, and strictly greater than 50% on the other attempts.  The reason is the same as for the coin flipping problem in Section \@ref(sim): in a fixed number of trials, the proportion of H on trials following H is expected to be less than the true probability of H, even though the trials are independent.  Therefore, for the example player a success rate of 50% on both sets of attempts actually provides directional evidence in favor of the hot hand.  Properly acccounting for this bias leads to substantially different statistical analyses (i.e., p-values) and conclusions.

## List of recurring examples

Add references/links to throughout text when book is finished.

```{example}
Roll a four-sided die^[Why four-sided?  Simply to make the number of possibilities a little more manageable (e.g., for in-class simulation activities).  Rolling a four-sided die twice yields 16 possible pairs, while rolling a six-sided die yields 36 possible pairs.] twice. What can we say about the sum of the two rolls?  The larger of the two rolls? This is an admittedly uninteresting example, but we will use this simple example to introduce many of the ideas in a relatively straightforward setting.
```

```{example}

```


